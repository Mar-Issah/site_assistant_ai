from langchain_core.prompts import ChatPromptTemplate
import os
#StrOutputParser convert llm response into string
from langchain_core.output_parsers import StrOutputParser
# RunnablePassthrough allow us pass in the fxn as a runnble so the chain can use it
from langchain_core.runnables import RunnablePassthrough
from langchain_community.utilities import SQLDatabase
from langchain_openai import  ChatOpenAI


template = """
Based on the table schema below, write a SQL query that would answer the users's question: {schema}

Question: {question}
SQL Query
"""
prompt = ChatPromptTemplate.from_template(template)
# print(prompt.format(schema="users", question="What is the name of the user with the id 1?"))

db_uri = os.getenv("DB_URI")
db = SQLDatabase.from_uri(db_uri)
# result = db.run("SELECT COUNT(ArtistId) as TotalArtists FROM artist;")
# print(result)

# method returns schema/table info. Creating the tables
def get_schema(_):
	return db.get_table_info()


# print(get_schema(None))
llm = ChatOpenAI()

#create sql chain
#RunnablePassthrough allows us to pass on the userâ€™s question to the prompt/template and model and get a response as a Str.
# by passing the schema=get_schema we have injected the schema into the prompt
#llm.bind(stop="\nSQL Result:") ensures that the model stops when it sees the SQL Result. good practice to prevent infinite loops/hallucinations.
sql_chain = (
	RunnablePassthrough.assign(schema=get_schema)
							| prompt
							| llm.bind(stop="\nSQL Result:")
							| StrOutputParser()
)

# now invoke it by passing in an obj with key-value pairs of variable to inject in the template
# print(sql_chain.invoke({"question": "What is the name of the user with the id 1?"}))

#output below
# maris@LAPTOP-GP06OUS5 MINGW64 /c/MARSIYA/PROGRAMMING/ALL_NOTES/AI/LANGCHAIN/LangChain_projects/site_assistant_ai (master)
# $ python sql_utils.py
# SELECT FirstName, LastName
# FROM customer
# WHERE CustomerId = 1;


# So now we want to run the query and Get a the final response by the llm using the schema, question, and response (query generated by invokng the runnable)
# template2 = """Based on the table schema below, question, sql query and sql response, write a natural language response that would answer the users's question:
# {schema}

# Question: {question}
# SQL Query: {query}
# SQL Response: {response}
# """
# we have removed the schema
template2 = """Based on the question, sql query and sql response, write a natural language response that would answer the users's question:

Question: {question}
SQL Query: {query}
SQL Response: {response}
"""

prompt2 = ChatPromptTemplate.from_template(template2)


def run_query(query):
	return db.run(query)


# print(run_query("SELECT COUNT(ArtistId) as TotalArtists FROM artist;")) answer is 275


# BASCIALLY RUNNING AND POPULATING THE INPUT_VARIBLES IN THE TEMPLATE FOR THE LLM TO BE ABLE TO EVALUATE AND REASON
# when chain is invoked with the question, it invokes the sql_chain with the question the chain runs and the retunr is saved in the query variable. the output which is a SQL query e.g 'SELECT * FROM customer', is  passed to second .assign which runs the get_schema fxn and saves it in the schema variable. Now we have the progress with each execution saved in their variables and everytime returning an object. The lambda fxn takes the objects and extracts the 'query' key which is a variable containing the query pass it to the run_query fxn to run it. We finally pass all the input_variables to the prompt2 and finally the llm for evaluation and reasoning
full_chain = (
	RunnablePassthrough
		.assign(query=sql_chain)
		.assign(
			# schema=get_schema,
		  response=lambda x: run_query(x['query']))
		| prompt2
		| llm
)

print(full_chain.invoke({"question": "What is the name of the user with the id 1?"}))